# LLM-VLM Planner - README

This folder provides a complete pipeline for robotic task planning from an image, using a language model (LLM) and a vision model (VLM), with integration of a ChromaDB vector database for RAG (Retrieval Augmented Generation).

## Folder structure

- `src/llm_vlm_planner/`: Main source code (planner, utilities, configuration).
    - `utils/`: Utility functions (processing, embeddings, etc).
    - `config/`: Prompts and configuration files.
    - `planners/`: Planner implementations.
- `main/`: Main launch scripts.
    - `planner_vlm_llm.py`: Main pipeline with user interaction (the user can correct or enrich the plan, but there is no direct exchange between the LLM and the VLM during planning).
    - `interactive_llm_vlm.py`: Advanced interactive script with double interaction: the user can correct/enrich the plan AND the LLM can dynamically query the VLM for details about the image (LLM <-> VLM <-> User).
- `experiment/`: Experimentation scripts (batch, ablation, etc).
- `Images/`: Input images for visual analysis (e.g., `device_live.png`).
- `audio/`: Audio files for tests or user feedback.
- `results/`: Generated results (plots, logs, etc).
    - `plot_*.png`: Automatically generated graphs.
    - `experiment_results.txt`, `results.txt`: Experiment logs and results.

## General workflow

1. **Image description**:
   - The VLM analyzes an image and returns the list of detected objects.
2. **Object-based planning**:
   - For each detected object, the LLM generates an action plan.
   - The user can refine the plan by adding information, which is indexed in ChromaDB.
   - The LLM can ask the VLM for details if needed ("ask vlm").
3. **RAG (Retrieval Augmented Generation)**:
   - Added documents are vectorized and stored in ChromaDB.
   - The LLM uses the most relevant documents to improve planning.
4. **Final plan generation**:
   - The planner (`TaskPlanner`) synthesizes the final answer from the last LLM exchange and useful documents.

## Usage

### Prerequisites
- Python 3.10+
- Dependencies:
  - `chromadb`
  - `ollama`
  - Modules from the `llm_vlm_planner` folder
- Required Ollama models installed locally (`qwen3:4b`, `qwen2.5vl`, etc.)

### Run the advanced interactive script (LLM <-> VLM <-> User)

```bash
python main/interactive_llm_vlm.py
```

- The script displays the image description and the list of detected objects.
- For each object, it proposes an action plan generated by the LLM.
- The user can add information (type text, then "no" to validate the plan).
- The LLM can dynamically ask the VLM for details (e.g.: "ask vlm: ...").
- At the end, the planner generates and displays the final plan for each object.

### Run the simple interactive pipeline (User <-> LLM)

```bash
python main/planner_vlm_llm.py
```

- The user can correct or enrich the plan proposed by the LLM for each object.
- There is no automatic interaction between the LLM and the VLM during planning (the VLM is only used for the initial image description).

### Run a batch experiment

```bash
python experiment/experiment_RAG.py
```
(or another script from the `experiment/` folder)

### Analyze the results

```bash
python results/analyze_log.py
```
The graphs will be saved in `results/`.

### Customization
- To change the analyzed image, modify the `image_path` variable in the scripts.
- To adjust prompts or logic, edit the functions in `src/llm_vlm_planner/utils/` or the configuration files in